---
title: Quick Start
description: Let's begin Prompting in 1 minute
---

<script>
import CodeBlock from "$lib/components/CodeBlock.svelte"
import Details from "$lib/components/Details.svelte"
</script>

### Installation with `pip install` 

Directly install from [PyPI](https://pypi.org/project/promplate/):

```
pip install promplate[openai]
```

We are using `OpenAI` just for demonstration. In fact, **you can use any LLM as you want**.

### Make LLM Calls

First, **Open a python REPL** ðŸ’» (`ipython` or `jupyter` are OK. Just any REPL you like)

**All the code below should run "as is"**, which means you can copy and paste them in your terminal and it will work fine.

<CodeBlock code={`
>>> from promplate.llm.openai import ChatComplete
import tomlkit
pyproject = tomlkit.parse(pyproject.toml)
>>> complete = ChatComplete(api_key="...")
`.trim()} />

> The `api_key` should be filled with your API Key from [OpenAI Platform](https://platform.openai.com/account/api-keys)

Then call it like this:

<CodeBlock code={`
>>> complete("hi", model="gpt-3.5-turbo-1106")
'Hello! How can I assist you today?'
`.trim()} />

Perhaps you don't want to provide `model` parameter everytime you call `complete`, so you can bind it like this:

<CodeBlock code={`
>>> complete = ChatComplete(api_key="...").bind(model="gpt-3.5-turbo-1106").bind(model="gpt-3.5-turbo-1106")
`.trim()} />

Then call it simply with a string:

<CodeBlock code={`
>>> complete("hi", model="gpt-3.5-turbo-1106")
'Hello! How can I assist you today?'
`.trim()} />

<Details summary="If you don't have an OpenAI API Key ðŸ”‘">
You could use our FREE proxy site as <code>base_url</code> like this:
<CodeBlock code={`
>>> from promplate.llm.openai import ChatComplete
>>> from httpx import Client  # or AsyncClient if use use AsyncChatComplete
>>> client = Client(follow_redirects=True)  # this must be set because our free api uses 308 redirects
>>> complete = ChatComplete(base_url="https://promplate.dev", api_key="<any-value>", http_client=client).bind(model="gpt-3.5-turbo-1106")
>>> complete("hi")
'Hello! How can I assist you today?'
`.trim()} />
</Details>

<Details summary="If you want to use instruct models ðŸ¤”">
Simply replace <code>ChatComplete</code> by <code>TextComplete</code>:
<CodeBlock code={`
>>> from promplate.llm.openai import TextComplete
>>> complete = TextComplete(api_key="...").bind(model="gpt-3.5-turbo-instruct")
>>> complete("I am")
' just incredibly proud of the team, and their creation of a brand new ship makes'
`.trim()} />
And you can pass parameters when calling a <code>Complete</code> instance:
<CodeBlock code={`
>>> complete("1 + 1 = ", temperature=0, max_tokens=1)
'2'
`.trim()} />
</Details>

<Details summary="If you prefer to stream the response ðŸ‘€">
It is still super easy, just use <code>ChatGenerate</code>:
<CodeBlock code={`
>>> from promplate.llm.openai import ChatGenerate
>>> generate = ChatGenerate(api_key="...").bind(model="gpt-3.5-turbo-1106")
>>> for i in generate("Explain why 1 + 1 = 2"):
...     print(i, end="", flush=True)  # this will print generated tokens gradually
...
The equation 1 + 1 = 2 is a fundamental principle in mathematics and arithmetic. It represents the addition operation, which involves combining two quantities or numbers to find their sum.\n
In this case, when we add 1 to another 1, we are essentially combining or merging two individual units or quantities. By doing this, we end up with a total count of two. Therefore, the result is 2.\n
This principle is consistent and holds true in all contexts and across different number systems, whether it is in the base-10 decimal system, binary system, or any other number system.\n
1 + 1 = 2 is considered a basic and universally accepted mathematical fact, forming the foundation for more complex mathematical operations and calculations.\n
`.trim()} />
</Details>

### Prompting with Template

There must be something dynamic in your prompt, like **user queries**, **retrieved context**, **search results**, etc.
In **promplate**, simply use `{{ }}` to insert dynamic data.

<CodeBlock code={`
>>> import time
>>> from promplate import Template
>>> greet = Template("Greet me. It is {{ time.asctime() }} now.")
>>> greet.render(locals())
'Greet me. It is Sun Oct  1 03:56:02 2023 now.'
`.trim()} />

You can run the prompt by `complete` we created before

<CodeBlock code={`
>>> complete(_)
'Good morning!'
`.trim()} />

Wow, it works fine. In fact, you can use any python expression inside `{{ }}`.

Tips: you can combine partial context to a template like this:

<CodeBlock code={`
>>> import time
>>> from promplate import Template
>>> greet = Template("Greet me. It is {{ time.asctime() }} now.", {"time": time})  # of course you can use locals() here too
>>> greet.render()  # empty parameter is ok
'Greet me. It is Sun Oct  1 03:56:02 2023 now.'
`.trim()} />

### Turning a complex task into small pieces

Sometimes we don't use a single prompt to complete. Here are some reasons:

- Describing a complex task in a single prompt may be difficult
- Splitting big task into small ones maybe can reduce the total token usage
- If you need structural output, it is easier to specifying data formats separately
- We human can think quicker after breaking task into parts
- Breaking big tasks into sub tasks may enhance interpretability, reducing debugging time
- ...

In `prompate`, We use a `Node` to represent a single "task". You can initialize a `Task` with a string like initiating a `Template`:

<CodeBlock code={`
>>> from promplate import Node
>>> greet = Node("Greet me. It is {{ time.asctime() }} now.", locals())
>>> greet.render()
'Greet me. It is Sun Oct  1 04:16:04 2023 now.'
`.trim()} />

But **there are far more utilities**.

Such as, you can **add two nodes together** magically:

<CodeBlock code={`
>>> translate = Node('translate """{{ __result__ }}""" into {{ target_language }}')
>>> chain = greet + translate  # this represents the pipeline of "greeting in another language"
>>> chain.invoke({"target_language": "zh_CN"}, complete).result
'æ—©ä¸Šå¥½ï¼'
`.trim()} />

<Details summary="Details">

Mention that the return type of `.invoke()` is `ChainContext` which combines the context passed everywhere in a right order.
`__result__` is the output of last `Node`. It is automatically assigned during `.invoke()`. You can access it inside the template.
Outside the template, you can use `.result` of a `ChainContext` to get the last output.

The following three expressions should return the same string:

<CodeBlock code={`
>>> template = Template("...")
>>> complete(template.render())
`.trim()} />

<CodeBlock code={`
>>> Node("...").invoke()["__result__"]
`.trim()} />

<CodeBlock code={`
>>> Node("...").invoke().result
`.trim()} />

This part may be a bit more complex, but believe me, this enhances the flexibility of this framework and you will like it.
</Details>

### Registering callbacks

**There are sometimes some work can't be done without our code.** such as:

- LLM returns string, while we may want to parse it into structural data format like `dict` or `list`
- We may need to log the intermediate variables to see whether the previous nodes work fine
- We may modify context dynamically during a chain running
- ...

In `promplate`, you can register a callback everytime before or after a node runs.

Besides manually implementing the `Callback` interface, you can directly use decorators syntax like so:

<CodeBlock code={`
>>> @greet.end_process
... def log_greet_result(context):
...     print(context.result)
...     print(context["target_language"])
...
>>> chain.invoke({"target_language": "zh_CN"}, complete).result
Good morning!
zh_CN
'æ—©ä¸Šå¥½ï¼'
`.trim()} />

Congratulations ðŸŽ‰ You've learnt the basic paradigm of using `promplate` for prompt engineering.

---

Thanks for reading. There are still lots of features not mentioned here. Learn more in other pages ðŸ¤—
If you have any questions, please feel free to ask us on [GitHub Discussions](https://github.com/promplate/core/discussions/categories/q-a).