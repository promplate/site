---
title: 〇门槛 快速上手
description: 一分钟速成提示工程！
---

<script>
import CodeBlock from "$lib/components/CodeBlock.svelte"
import Details from "$lib/components/Details.svelte"
</script>

### 直接 `pip install` 安装 🌟

从 [PyPI](https://pypi.org/project/promplate/) 安装：

```
pip install promplate[openai]
```

你也可以用任何你喜欢的源（比如以清华源为例）：

```
pip install promplate[openai] -i https://pypi.tuna.tsinghua.edu.cn/simple
```

我们只是用 `OpenAI` 来做演示。实际上，**你可以使用任何你想要的LLM**。

### 让LLM生成文本！OpenAI 启动！

首先，**打开一个python REPL** 💻 （或者`ipython`或者`jupyter`都可以。**任何你喜欢的python终端都可以**）

**下面的所有代码都可以直接运行**，只要你是一行行复制粘贴过去的，都可以正常得出结果。

<CodeBlock code={`
>>> from promplate.llm.openai import ChatComplete  # 这个类只是对OpenAI的官方SDK的封装而已
>>> complete = ChatComplete(model="gpt-3.5-turbo-1106", api_key="...")
`.trim()} />

> 在 `api_key` 填你的 [API Key](https://platform.openai.com/account/api-keys) 就行

然后只需用一个字符串来调用它：

<CodeBlock code={`
>>> complete("hi")
'Hello! How can I assist you today?'
`.trim()} />

<Details summary="😭😭 如果你没有 API Key">
我有个教育目的的免费代理站，你不用 API Key 也能免费使用。像这样填写 <code>api_base</code> 字段即可：
<CodeBlock code={`
>>> from promplate.llm.openai import ChatComplete
>>> complete = ChatComplete(model="gpt-3.5-turbo-1106", api_base="https://promplate.dev")
>>> complete("hi")
'Hello! How can I assist you today?'
`.trim()} />
</Details>

<Details summary="如果你想使用 instruct 模型 🤔">
只需简单地将 <code>ChatComplete</code> 替换为 <code>TextComplete</code>：
<CodeBlock code={`
>>> from promplate.llm.openai import TextComplete
>>> complete = TextComplete(model="gpt-3.5-turbo-instruct", api_key="...")
>>> complete("I am")
' just incredibly proud of the team, and their creation of a brand new ship makes'
`.trim()} />
当调用 <code>Complete</code> 实例时，你可以传递参数，比如 temperature，也可以在初始化时修改参数：
<CodeBlock code={`
>>> complete("1 + 1 = ", temperature=0, max_tokens=1)
'2'
`.trim()} />
</Details>

<Details summary="想迭代流式响应也很简单 👀">
这仍然非常简单，只需使用 <code>ChatGenerate</code>：
<CodeBlock code={`
>>> from promplate.llm.openai import ChatGenerate
>>> generate = ChatGenerate(model="gpt-3.5-turbo-1106", api_key="...")
>>> for i in generate("Explain why 1 + 1 = 2"):
...     print(i, end="", flush=True)  # 这会逐渐打印生成的token
...
The equation 1 + 1 = 2 is a fundamental principle in mathematics and arithmetic. It represents the addition operation, which involves combining two quantities or numbers to find their sum.\n
In this case, when we add 1 to another 1, we are essentially combining or merging two individual units or quantities. By doing this, we end up with a total count of two. Therefore, the result is 2.\n
This principle is consistent and holds true in all contexts and across different number systems, whether it is in the base-10 decimal system, binary system, or any other number system.\n
1 + 1 = 2 is considered a basic and universally accepted mathematical fact, forming the foundation for more complex mathematical operations and calculations.\n
`.trim()} />
</Details>

### 模板！如何使用 Promplate 的模板表示动态的 prompt

在你的提示中必然有一些动态的内容，比如**用户输入的问题**，**从数据库中检索到的数据**，**网上搜索的结果**等。
在 **promplate** 中，只需使用 `{{ }}` 就可以插入动态数据。

<CodeBlock code={`
>>> import time
>>> from promplate import Template
>>> greet = Template("Greet me. It is {{ time.asctime() }} now.")
>>> greet.render(locals())
'Greet me. It is Sun Oct  1 03:56:02 2023 now.'
`.trim()} />

借助刚刚创建的 `complete` 对象来执行它：

<CodeBlock code={`
>>> complete(_)  # _ 在 REPL 中就是上一个表达式的值
'Good morning!'
`.trim()} />

可以看到，它根据我们的当前时间给出了合适的问候语。

实际上，你可以在 `{{ }}` 里面使用任何python表达式。

### 问题有点复杂？分解成子问题逐个解决！

有时候需要组合多个 prompt 来完成任务：

- 如果任务比较复杂，可能很难在一个 prompt 中描述清楚
- 将大任务分解成小任务分别执行有可能可以减少 token 消耗量
- 如果你要让模型输出结构化的数据（通过XML或者JSON），往往先生成正常结果再
- 将任务分解成部分来完成是符合人类直觉的
- 将大任务分解成子任务可能会增强可解释性，debug 时更容易定位错误
- ...

在 `prompate` 中，我们使用 `Node` 来表示一个单一的"任务"。你可以用一个字符串来初始化一个 `Task`：

<CodeBlock code={`
>>> from promplate import Node
>>> greet = Node("Greet me. It is {{ time.asctime() }} now.")
>>> greet.render(locals())
'Greet me. It is Sun Oct  1 04:16:04 2023 now.'
`.trim()} />

一个 `Node` 就是一个加了**许多实用功能**的 `Template` ：

<CodeBlock code={`
>>> greet = Node("Greet me. It is {{ time.asctime() }} now.", locals())
>>> greet.render()  # 你已经在这个 Node 初始化时传递了上下文
'Greet me. It is Sun Oct  1 04:16:54 2023 now.'
`.trim()} />

像魔法一样，你可以直接**将两个节点相加**，表示先后连接的顺序：

<CodeBlock code={`
>>> translate = Node('translate """{{ __result__ }}""" into {{ target_language }}')
>>> chain = greet + translate  # 这表示了"用中文打招呼"的流程
>>> chain.run({"target_language": "zh_CN"}, complete).result
'早上好！'
`.trim()} />

注意 `.run()` 的返回类型是 `ChainContext`，这是一个 `dict` 的子类，但是比 `dict` 多了一些东西：

- `__result__` 键在 `.run()` 时被自动分配为上一个 `Node` 的输出，你可以在模板中用 `{{ __result__ }}` 访问它；

- 在模板外部，你可以使用 `ChainContext` 的 `.result` 来获取最后的输出。

以下三个表达式应该返回相同的字符串：

<CodeBlock code={`
>>> template = Template("...")
>>> complete(template.render({}))
`.trim()} />

<CodeBlock code={`
>>> Node("...").run({})["__result__"]
`.trim()} />

<CodeBlock code={`
>>> Node("...").run({}).result
`.trim()} />

这部分可能有点难理解。这是为了这个框架的灵活性。事实上，等你的流水线越来越长了，你会发现这就是最佳实践。

### Agent = LLM + Tools! 给 `Chain` 注册回调！

有些事情LLM做不了，更适合用代码来完成。例如：

- LLM只会返回字符串，但我们有时需要将其解析为结构化的数据格式，如 `dict` 或 `list`
- `Chain` 默认只保留最新的 `__result__`，而可能也需要保留下来中间的 `Node` 的输出
- 后一个 `Node` 可能不能直接使用前一个 `Node` 的结果，而是需要进行一些处理
- ...

在 `promplate` 中，你可以在 LLM 运行前后分别注册一个或多个回调，分别称为"预处理"和"后处理"

除了在 `Node` 初始化时传入，还可以用装饰器的方式注册回调：

<CodeBlock code={`
>>> @greet.post_process
... def log_greet_result(context):
...     print(context.result)
...     print(context["target_language"])
...
>>> chain.run({"target_language": "zh_CN"}, complete).result
Good morning!
zh_CN
'早上好！'
`.trim()} />

上面这是一个将第一个 `Node` 的结果 print 出来的回调。事实上，你可以在回调中随意读取和写入 `context`

---

恭喜你 🎉 你已经学会了使用 `promplate` 进行提示工程的基本范式。感谢你的阅读！还有很多没有提到的特性在这个速成教程中没有提到。你可以前往其它页面了解更多 🤗
